---
title: Importing additional spatial data
format: html
editor: source

execute:
  echo: true
---

```{r}
library(sf)
library(mapview)
library(dplyr)
```

## Mined Units 

The mined units can be read in directly with `st_read`. 

```{r}
mined <- st_read('https://ca.dep.state.fl.us/arcgis/rest/services/OpenData/MMP_MINEDUNITS/MapServer/0/query?outFields=*&where=1%3D1&f=geojson')
mapview(mined)
```

## Brownfield sites

Download the zipped kml file to a temporary directory.

```{r}
# url with zipped kml
urlin <- 'https://ordsext.epa.gov/FLA/www3/acres_frs.kmz'

# download file
tmp1 <- tempfile(fileext = ".kmz")
download.file(url = urlin, destfile = tmp1, method = 'curl')
```

Unzip the kmz file.

```{r}
tmp2 <- tempdir()
unzip(tmp1, exdir = tmp2)
```

Get the name of the kml file to read. 

```{r}
lyr <- unzip(tmp1, list = T)$Name
fl <- paste(c(tmp2, lyr), collapse = "\\")
fl <- gsub('\\\\', '/', fl)
```

Read the kml file with `st_read` and drop Z dimension with `st_zm`.  Here, the Tampa sites are loaded.  You can view all possible locations in the kml file with `st_layers`.

```{r}
dat <- st_read(fl, layer = 'TAMPA') %>% 
  st_zm()
mapview(dat)
```

To import all layers in the kml file, identify the layer names and loop through them to add to a single object.  This takes several hours. The data are saved as an [.Rdata object](https://github.com/tbep-tech/equity-plan-mapping/raw/main/data/allbfld.RData) and [.csv file](https://github.com/tbep-tech/equity-plan-mapping/raw/main/data/dattb.csv) for later use.  The data include only the site name and location in decimal degrees.

```{r}
#| eval: false
# layer names
alllyr <- st_layers(fl)$name

strt <- Sys.time()
out <- NULL
for(i in alllyr){
  
  # counter
  cat(i, which(i == alllyr), 'of', length(alllyr), '\n')
  print(Sys.time() - strt)
  
  # import each layer
  dat <- st_read(fl, i, quiet = T)[, c('Name')]
  
  # append to same object
  out <- rbind(out, dat)
  
}

# save as RData object
allbfld <- out %>% st_zm()
save(allbfld, file = 'data/allbfld.RData', compress = 'xz')

# save as csv
allbfldcsv <- allbfld %>% 
  mutate(
    lon = st_coordinates(.)[,1], 
    lat = st_coordinates(.)[,2]
  ) %>% 
  st_set_geometry(NULL)
write.csv(allbfldcsv, 'data/allbfldcsv.csv', row.names = F)
```

View all brownfield sites.

```{r}
load(file = 'data/allbfld.RData')
mapview(allbfld, legend = F, col.regions = 'grey')
```

Unlink the temporary files to delete them when you are finished.

```{r}
unlink(tmp1, recursive = TRUE)
unlink(fl, recursive = TRUE)
```

## Census tracts

This workflow demonstrates how to download census tracts for the United States.

Download the zip file to a temporary directory and unzip to a second temporary directory.

```{r}
#| cache: true
urlin <- 'https://static-data-screeningtool.geoplatform.gov/data-versions/1.0/data/score/downloadable/1.0-shapefile-codebook.zip'

tmp1 <- tempfile(fileext = ".zip")
download.file(url = urlin, destfile = tmp1)

tmp2 <- tempdir()
unzip(tmp1, exdir = tmp2)
```

The `usa.zip` file was included in the previously zipped file.  Unzip `usa.zip` file. 

```{r}
zip1 <- list.files(tmp2, 'usa\\.zip', full.names = T)
unzip(zip1, exdir = tmp2)
```

Get the file path for `usa.shp` and import with the sf package. 

```{r}
fl <- list.files(tmp2, '\\.shp', full.names = T)
tracts <- st_read(fl)
```

Clip to the Tampa Bay watershed boundaries using the `tbshed` spatial object. The CRS is the same, so there is no need to transform. 

```{r}
#| cache: true
load(file = 'data/tbshed.RData')

tbtracts <- tracts %>% 
  st_intersection(tbshed)

save(tbtracts, file = 'data/tbtracts.RData')
```

View the data. 

```{r}
mapview(tbtracts)
```

Remove temporary files.

```{r}
unlink(tmp1, recursive = TRUE)
unlink(zip1)
fls <- list.files(tmp2, gsub('\\.shp$', '', basename(fl)), full.names = T)
file.remove(fls)
```

